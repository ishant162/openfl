{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def inference(network,test_loader):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "        output = network(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * correct / len(test_loader.dataset)))\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def FedAvg(models, weights=None):\n",
    "    new_model = models[0]\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    state_dict = new_model.state_dict()\n",
    "    for key in models[1].state_dict():\n",
    "        state_dict[key] = torch.from_numpy(np.average([state[key].numpy() for state in state_dicts],\n",
    "                                                      axis=0, \n",
    "                                                      weights=weights))\n",
    "    new_model.load_state_dict(state_dict)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from openfl.experimental.interface import FLSpec\n",
    "from openfl.experimental.placement import aggregator, collaborator\n",
    "\n",
    "\n",
    "class FederatedFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model=None, optimizer=None, rounds=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                                       momentum=momentum)\n",
    "        self.rounds = rounds\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        print(f'Performing initialization for model')\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        self.current_round = 0\n",
    "        self.next(self.aggregated_model_validation, foreach='collaborators', exclude=['private'])\n",
    "\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        print(f'Performing aggregated model validation for collaborator {self.input}')\n",
    "        self.agg_validation_score = inference(self.model, self.test_loader)\n",
    "        print(f'{self.input} value of {self.agg_validation_score}')\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                                   momentum=momentum)\n",
    "        train_losses = []\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: 1 [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                    100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "                self.loss = loss.item()\n",
    "                torch.save(self.model.state_dict(), 'model.pth')\n",
    "                torch.save(self.optimizer.state_dict(), 'optimizer.pth')\n",
    "        self.training_completed = True\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        self.local_validation_score = inference(self.model, self.test_loader)\n",
    "        print(\n",
    "            f'Doing local model validation for collaborator {self.input}: {self.local_validation_score}')\n",
    "        self.next(self.join, exclude=['training_completed'])\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        self.average_loss = sum(input.loss for input in inputs) / len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(\n",
    "            input.agg_validation_score for input in inputs) / len(inputs)\n",
    "        self.local_model_accuracy = sum(\n",
    "            input.local_validation_score for input in inputs) / len(inputs)\n",
    "        print(f'Average aggregated model validation values = {self.aggregated_model_accuracy}')\n",
    "        print(f'Average training loss = {self.average_loss}')\n",
    "        print(f'Average local model validation values = {self.local_model_accuracy}')\n",
    "        self.model = FedAvg([input.model for input in inputs])\n",
    "        self.optimizer = [input.optimizer for input in inputs][0]\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next(self.aggregated_model_validation,\n",
    "                      foreach='collaborators', exclude=['private'])\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        print(f'This is the end of the flow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "best_model = None\n",
    "optimizer = None\n",
    "# FLSpec has access to runtime object, it will send following arguments to runtime to build plan.yaml file\n",
    "flflow = FederatedFlow(model, optimizer, rounds=2, checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.experimental.interface import Federation\n",
    "from openfl.experimental.runtime import FederatedRuntime\n",
    "from openfl.experimental.interface import Aggregator\n",
    "from openfl.experimental.interface import Collaborator\n",
    "\n",
    "# 2. Connect to director and run service\n",
    "# 3. Connect to envoys and run service\n",
    "federation = Federation(\n",
    "    director={\n",
    "        \"fqdn\": \"fqdn\",\n",
    "        \"port\": 1008,\n",
    "        \"user\": \"ubuntu\",\n",
    "        \"password\": \"password for director machine\",\n",
    "        \"identity file\": \"password OR identity file for this director\",\n",
    "    },\n",
    "    envoys={\n",
    "        \"env1\": {\n",
    "            \"fqdn\": \"envoy fqdn\",\n",
    "            \"port\": 1001,\n",
    "            \"username\": \"ubuntu\",\n",
    "            \"password\": \"password for envoy 1\",\n",
    "            \"identity_file\": \"password OR identity file for this envoy\",\n",
    "            \"private_attributes\": \"env1_pa\",\n",
    "        },\n",
    "    },\n",
    "    tls=False,\n",
    "    client_id=\"client_id\",\n",
    "    cert_chain=\"path to cert chain\",\n",
    "    api_cert=\"path to api cert\",\n",
    "    api_private_key=\"path to api private key\",\n",
    ")\n",
    "\n",
    "agg = Aggregator()\n",
    "collaborators = [Collaborator() for _ in range(2)]\n",
    "\n",
    "# Job of Runtime is defined as below:\n",
    "# 1. Populate plan.yaml, data.yaml\n",
    "# 4. Provide metrics from aggregator -> director -> experiment mgr (here)\n",
    "flflow.runtime = FederatedRuntime(\n",
    "    aggregator=agg,\n",
    "    collaborators=collaborators,\n",
    "    federation=federation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare 2 zip files for 1 for aggregator, and 1 for collaborator\n",
    "# aggregator zip will include:\n",
    "    # flow.py (this file)\n",
    "    # file mentioned in aggregator_private_attributes\n",
    "    # plan.yaml\n",
    "    # data.yaml\n",
    "# collaborator zip will include:\n",
    "    # file mentioned in private_attributes_filepath\n",
    "    # plan.yaml\n",
    "    # data.yaml\n",
    "# 2. Submit experiment zip files to director\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLSpec will do this via runtime object provided to it.\n",
    "flflow.stream_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get status of running experiment. Will use runtime object to get the latest status.\n",
    "flflow.experiment_status()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-openfl-parth-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
